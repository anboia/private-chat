{
  "dashboard": {
    "id": null,
    "title": "OpenAI Proxy Metrics",
    "description": "Monitoring dashboard for OpenAI Proxy server",
    "tags": ["openai", "proxy", "api"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate (req/sec)",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_requests_total[5m]))",
            "legendFormat": "Requests/sec"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 6,
          "x": 0,
          "y": 0
        },
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Average Response Time",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_request_duration_seconds_sum[5m])) / sum(rate(openai_proxy_request_duration_seconds_count[5m]))",
            "legendFormat": "Avg Response Time"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 6,
          "x": 6,
          "y": 0
        },
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 3,
        "title": "Active Requests",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(openai_proxy_active_requests)",
            "legendFormat": "Active"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 6,
          "x": 12,
          "y": 0
        },
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 4,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_requests_total{status_code!~\"2..\"}[5m]))",
            "legendFormat": "Errors/sec"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 6,
          "x": 18,
          "y": 0
        },
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "color": {
              "mode": "palette-classic"
            },
            "thresholds": {
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "yellow",
                  "value": 0.1
                },
                {
                  "color": "red",
                  "value": 1
                }
              ]
            }
          }
        }
      },
      {
        "id": 5,
        "title": "Request Rate by Endpoint",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_requests_total[5m])) by (endpoint)",
            "legendFormat": "{{endpoint}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 8
        },
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 6,
        "title": "Response Time Percentiles",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum(rate(openai_proxy_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.90, sum(rate(openai_proxy_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "90th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(openai_proxy_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "99th percentile"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 8
        },
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 7,
        "title": "Token Usage Rate",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_tokens_total{type=\"prompt\"}[5m]))",
            "legendFormat": "Prompt tokens/sec"
          },
          {
            "expr": "sum(rate(openai_proxy_tokens_total{type=\"completion\"}[5m]))",
            "legendFormat": "Completion tokens/sec"
          },
          {
            "expr": "sum(rate(openai_proxy_tokens_total{type=\"total\"}[5m]))",
            "legendFormat": "Total tokens/sec"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 16
        },
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 8,
        "title": "Cache Performance",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_cache_operations_total{result=\"hit\"}[5m]))",
            "legendFormat": "Cache hits/sec"
          },
          {
            "expr": "sum(rate(openai_proxy_cache_operations_total{result=\"miss\"}[5m]))",
            "legendFormat": "Cache misses/sec"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 16
        },
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 9,
        "title": "Cache Hit Ratio",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_cache_operations_total{result=\"hit\"}[5m])) / sum(rate(openai_proxy_cache_operations_total[5m])) * 100",
            "legendFormat": "Hit Ratio"
          }
        ],
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 0,
          "y": 24
        },
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "color": {
              "mode": "palette-classic"
            },
            "thresholds": {
              "steps": [
                {
                  "color": "red",
                  "value": null
                },
                {
                  "color": "yellow",
                  "value": 50
                },
                {
                  "color": "green",
                  "value": 80
                }
              ]
            }
          }
        }
      },
      {
        "id": 10,
        "title": "OpenAI API Errors",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(openai_proxy_openai_errors_total[5m])) by (error_type)",
            "legendFormat": "{{error_type}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 18,
          "x": 6,
          "y": 24
        },
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 11,
        "title": "Status Code Distribution",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum(increase(openai_proxy_requests_total[5m])) by (status_code)",
            "legendFormat": "{{status_code}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 32
        },
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      },
      {
        "id": 12,
        "title": "Request Volume by Client",
        "type": "timeseries",
        "targets": [
          {
            "expr": "topk(10, sum(rate(openai_proxy_requests_total[5m])) by (client_key))",
            "legendFormat": "{{client_key}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 32
        },
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "color": {
              "mode": "palette-classic"
            }
          }
        }
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s",
    "schemaVersion": 39,
    "version": 1,
    "weekStart": ""
  }
}